{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDKmrDSfq6L7"
      },
      "source": [
        "<img src=\"https://www.dropbox.com/s/fchpltm5rnwd5ce/Flatiron%20Logo%202Wordmark.png?raw=1\" width=100 >\n",
        "\n",
        "# Web Scraping 101\n",
        "- James M. Irving, Ph.D.\n",
        "- james.irving.phd@gmail.com\n",
        "- Repo: https://github.com/jirvingphd/my_data_science_notes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqLBiwwB6I2k"
      },
      "source": [
        "# Quick Google Colab Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_zdpwZ6IfO"
      },
      "source": [
        "**Google Colab Quick - Notes**\n",
        " 1. Open the sidebar! (the little  ' > ' button on the top-left of the document pane.)\n",
        "    - Use `Table of Contents` to Jump between the 3 major sections.\n",
        "    - Mount your google drive via the `Files `\n",
        "    - Note: to make a section appear in the Table of Contents, create a NEW text cell for the *header only*. This will also let you collapse all of the cells in the section to reduce clutter.\n",
        "\n",
        "    \n",
        " 2. Google Colab already has most common python packages.\n",
        "    - You can pip install anything by prepending an exclamation point\n",
        "    ```python\n",
        "    !pip install bs_ds\n",
        "    !pip install fake_useragent\n",
        "    !pip install lxml\n",
        "    ```\n",
        "    \n",
        "3. Open a notebook from github or save to github using `File > Upload Notebook` and `File> Save a copy in github`, respectively\n",
        "\n",
        "4. Using GPUs/TPUs\n",
        "    - `Runtime > Change Runtime Type > Hardware Acceleration`\n",
        "\n",
        "5. Run-Before and Run-After\n",
        "    - Go to `Runtime` and select `Run before` to run all cells up to the currently active cell\n",
        "    - Go to `Runtime` and select `Run after` to run all cells that follow the currently active cell\n",
        "\n",
        "6. Cloud Files with Colab\n",
        "    - **Open .csv's stored in a github repo directly with Pandas**:\n",
        "        - Go to the repo on GitHub, click on the csv file, then click on `Download` or `Raw` which will then change to show you the raw text. Copy and paste the link in your address bar (should start with www.rawgithubusercontent).\n",
        "        - In your notebook, do `df=pd.read_csv(url)` to load in the data.\n",
        "    - **Google Drive: Open sidebar > Files> click Mount Drive**\n",
        "        - or use this function:\n",
        "        ```python\n",
        "        def mount_google_drive(force_remount=True):\n",
        "            from google.colab import drive\n",
        "            print('drive_filepath=\"drive/My Drive/\"')\n",
        "            return drive.mount('/content/drive', force_remount=force_remount)\n",
        "        ```\n",
        "        - Then access files by file path like usual.\n",
        "        \n",
        "    - Dropbox Files: (like images or csv)\n",
        "        - Copy and paste the share link.\n",
        "        - Change the end of the link from `dl=0`to `dl=1`\n",
        "        \n",
        "6B. Function To Turn Google Drive Share links into usable image links for html\n",
        "\n",
        "```python\n",
        "def make_gdrive_file_url(share_url_from_gdrive):\n",
        "    \"\"\"accepts gdrive share url with format 'https://drive.google.com/open?id=`\n",
        "    and returns a pandas-usable link with format ''https://drive.google.com/uc?export=download&id='\"\"\"\n",
        "    import re\n",
        "    file_id = re.compile(r'id=(.*)')\n",
        "    fid = file_id.findall(share_url_from_gdrive)\n",
        "    prepend_url = 'https://drive.google.com/uc?export=download&id='\n",
        "    output_url = prepend_url + fid[0]\n",
        "    return output_url\n",
        "\n",
        "test_link = \"https://drive.google.com/open?id=1eHbOq-2TqGx4d2jZXrUdwNnJY_aM_7rj\" # airline passenger .csv\n",
        "file_link = make_gdrive_file_url(test_link)\n",
        "file_link\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAuVqqH66xeI"
      },
      "source": [
        "# Web Scraping 101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wnkf6zBwjsF"
      },
      "source": [
        "**Table of Contents - Shallow**\n",
        "1. Notes on Using BeautifulSoup\n",
        "2. Walk-through example/code\n",
        "    - My personal functions and then a working code frame using them.\n",
        "3. Notes Section for\n",
        " - After this, make sure to check out [Brandon's Web Scraping 202](https://github.com/cyranothebard/flatironschool_datascience/blob/master/Web%20Scraping%20202.ipynb)\n",
        " - He goes into using alternating ip addresses and more complex framework for harvesting content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Recommended packages/tools to use\n",
        "1. `fake_useragent`\n",
        "    - pip-installable module that conveniently supplies fake user agent information to use in your request headers.\n",
        "    - recommended by udemy course\n",
        "2. `lxml`\n",
        "    - popular pip installable html parser (recommended by Udemy course)\n",
        "    - using `'html.parser'` in requests.get() did not work for me, I had to install lxml\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "gh6WTxiVYqkh"
      },
      "outputs": [],
      "source": [
        "# !pip install bs_ds\n",
        "# !pip install fake_useragent\n",
        "# !pip install lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaWIHHicwtAr"
      },
      "source": [
        "## Using python's `requests` module:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JpBMzWysPb0"
      },
      "source": [
        "\n",
        "-  Use `requests` library to initiate connections to a website.\n",
        "- Check the status code returned to determine if connection was successful (status code=200)\n",
        "\n",
        "```python\n",
        "import requests\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "\n",
        "# Connect to the url using requests.get\n",
        "response = requests.get(url)\n",
        "response.status_code\n",
        "```\n",
        "\n",
        " ___\n",
        "| Status Code | Code Meaning\n",
        "| --------- | -------------|\n",
        "1xx |   Informational\n",
        "2xx|    Success\n",
        "3xx|     Redirection\n",
        "4xx|     Client Error\n",
        "5xx |    Server Error\n",
        "\n",
        "___\n",
        "- **Note: You can add a `timeout` to `requests.get()` to avoid indefinite waiting**\n",
        "    - Best in multiples of 3 (`timeout=3` or `6` , `9` ,etc.)\n",
        "\n",
        "```python\n",
        "# Add a timeout to prevent hanging\n",
        "response = requests.get(url, timeout=3)\n",
        "response.status_code\n",
        "\n",
        "```\n",
        "- **`response` is a dictionary with the contents printed below**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "w5Rn-Y0N0q3B",
        "outputId": "eb7b1af9-2a9a-447a-cb7b-afcd1ce72ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status code:  200\n",
            "Connection successfull.\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\tContents of Response.items():\n",
            "------------------------------------------------------------\n",
            "date                     : Fri, 26 Jul 2024 02:51:38 GMT           \n",
            "server                   : mw-web.codfw.main-5984ddd686-w95m7      \n",
            "x-content-type-options   : nosniff                                 \n",
            "content-language         : en                                      \n",
            "origin-trial             : AonOP4SwCrqpb0nhZbg554z9iJimP3DxUDB8V4yu9fyyepauGKD0NXqTknWi4gnuDfMG6hNb7TDUDTsl0mDw9gIAAABmeyJvcmlnaW4iOiJodHRwczovL3dpa2lwZWRpYS5vcmc6NDQzIiwiZmVhdHVyZSI6IlRvcExldmVsVHBjZCIsImV4cGlyeSI6MTczNTM0Mzk5OSwiaXNTdWJkb21haW4iOnRydWV9\n",
            "accept-ch                :                                         \n",
            "vary                     : Accept-Encoding,Cookie,Authorization    \n",
            "last-modified            : Thu, 25 Jul 2024 21:14:16 GMT           \n",
            "content-type             : text/html; charset=UTF-8                \n",
            "content-encoding         : gzip                                    \n",
            "age                      : 534                                     \n",
            "x-cache                  : cp4037 miss, cp4037 hit/4               \n",
            "x-cache-status           : hit-front                               \n",
            "server-timing            : cache;desc=\"hit-front\", host;desc=\"cp4037\"\n",
            "strict-transport-security: max-age=106384710; includeSubDomains; preload\n",
            "report-to                : { \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }\n",
            "nel                      : { \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}\n",
            "set-cookie               : WMF-Last-Access=26-Jul-2024;Path=/;HttpOnly;secure;Expires=Tue, 27 Aug 2024 00:00:00 GMT, WMF-Last-Access-Global=26-Jul-2024;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Tue, 27 Aug 2024 00:00:00 GMT, WMF-DP=9b6;Path=/;HttpOnly;secure;Expires=Fri, 26 Jul 2024 00:00:00 GMT, GeoIP=US:CA:Canyon_Country:34.43:-118.45:v4; Path=/; secure; Domain=.wikipedia.org, NetworkProbeLimit=0.001;Path=/;Secure;SameSite=Lax;Max-Age=3600\n",
            "x-client-ip              : 172.249.59.245                          \n",
            "cache-control            : private, s-maxage=0, max-age=0, must-revalidate\n",
            "accept-ranges            : bytes                                   \n",
            "content-length           : 78120                                   \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "\n",
        "response = requests.get(url, timeout=3)\n",
        "print('Status code: ',response.status_code)\n",
        "if response.status_code==200:\n",
        "    print('Connection successfull.\\n\\n')\n",
        "else:\n",
        "    print('Error. Check status code table.\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "# Print out the contents of a request's response\n",
        "print(f\"{'---'*20}\\n\\tContents of Response.items():\\n{'---'*20}\")\n",
        "\n",
        "for k,v in response.headers.items():\n",
        "    print(f\"{k:{25}}: {v:{40}}\") # Note: add :{number} inside of a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "fbpLnJrzDetE",
        "outputId": "c1b0484f-4902-4114-ea8a-74d83623a5e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "date: Fri, 26 Jul 2024 02:51:38 GMT\n",
            "server: mw-web.codfw.main-5984ddd686-w95m7\n",
            "x-content-type-options: nosniff\n",
            "content-language: en\n",
            "origin-trial: AonOP4SwCrqpb0nhZbg554z9iJimP3DxUDB8V4yu9fyyepauGKD0NXqTknWi4gnuDfMG6hNb7TDUDTsl0mDw9gIAAABmeyJvcmlnaW4iOiJodHRwczovL3dpa2lwZWRpYS5vcmc6NDQzIiwiZmVhdHVyZSI6IlRvcExldmVsVHBjZCIsImV4cGlyeSI6MTczNTM0Mzk5OSwiaXNTdWJkb21haW4iOnRydWV9\n",
            "accept-ch: \n",
            "vary: Accept-Encoding,Cookie,Authorization\n",
            "last-modified: Thu, 25 Jul 2024 21:14:16 GMT\n",
            "content-type: text/html; charset=UTF-8\n",
            "content-encoding: gzip\n",
            "age: 534\n",
            "x-cache: cp4037 miss, cp4037 hit/4\n",
            "x-cache-status: hit-front\n",
            "server-timing: cache;desc=\"hit-front\", host;desc=\"cp4037\"\n",
            "strict-transport-security: max-age=106384710; includeSubDomains; preload\n",
            "report-to: { \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }\n",
            "nel: { \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}\n",
            "set-cookie: WMF-Last-Access=26-Jul-2024;Path=/;HttpOnly;secure;Expires=Tue, 27 Aug 2024 00:00:00 GMT, WMF-Last-Access-Global=26-Jul-2024;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Tue, 27 Aug 2024 00:00:00 GMT, WMF-DP=9b6;Path=/;HttpOnly;secure;Expires=Fri, 26 Jul 2024 00:00:00 GMT, GeoIP=US:CA:Canyon_Country:34.43:-118.45:v4; Path=/; secure; Domain=.wikipedia.org, NetworkProbeLimit=0.001;Path=/;Secure;SameSite=Lax;Max-Age=3600\n",
            "x-client-ip: 172.249.59.245\n",
            "cache-control: private, s-maxage=0, max-age=0, must-revalidate\n",
            "accept-ranges: bytes\n",
            "content-length: 78120\n"
          ]
        }
      ],
      "source": [
        "for k,v in response.headers.items():\n",
        "    print(f\"{k}: {v}\") # Note: add :{number} inside of a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAPbGwxTDZbG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjXVI4Qpw2_n"
      },
      "source": [
        "## Random Tips - Text Printing/Formatting:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkwN3WP80tiS"
      },
      "source": [
        "\n",
        "- **You can repeat strings by using multiplication**\n",
        "    - `'---'*20` will repeat the dashed lines 20 times\n",
        "\n",
        "- **You can determine how much space is alloted for a variable when using f-strings**\n",
        "    - Add a `:{##}` after the variable to specify the allocated width\n",
        "    - Add a `>` before the `{##}` to force alignment\n",
        "    - Add another symbol (like '.'' or '-') before `>` to add guiding-line/placeholder (like in a table of contents)\n",
        "\n",
        "```python\n",
        "print(f\"Status code: {response.status_code}\")\n",
        "print(f\"Status code: {response.status_code:>{20}}\")\n",
        "print(f\"Status code: {response.status_code:->{20}}\")\n",
        "```    \n",
        "```\n",
        "# Returns:\n",
        "Status code: 200\n",
        "Status code:                  200\n",
        "Status code: -----------------200\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO3ac1hE8gr5"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSwUhVVTwvRq"
      },
      "source": [
        "## Quick Review -  HTML & Tags\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jZuk4QQ17nU"
      },
      "source": [
        "- All HTML pages have the following components\n",
        "    1. document declaration followed by html tag\n",
        "    \n",
        "    `<!DOCTYPE html>`<br>\n",
        "    `<html>`\n",
        "    2. Head\n",
        "     html tag<br>\n",
        "    `<head> <title></title></head>`\n",
        "    3. Body<br>\n",
        "    `<body>` ... content... `</body>`<br>\n",
        "    `</html>`\n",
        "\n",
        "- Html content is divdied into **tags** that specify the type of content.\n",
        "    - [Basic Tags Reference Table](https://www.w3schools.com/tags/ref_byfunc.asp)\n",
        "    - [Full Alphabetical Tag Reference Table](https://www.w3schools.com/tags/)\n",
        "    \n",
        "    - **tags** have attributes\n",
        "        - [Tag Attributes](https://www.w3schools.com/html/html_attributes.asp)\n",
        "        - Attributes are always defined in the start/opening tag.\n",
        "\n",
        "    - **tags** may have several content-creator-defined attributes such as `class` or `id`\n",
        "- We will **use the tag and its identifying attributes to isolate content** we want on a web page with BeautifulSoup.\n",
        "\n",
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7H_4da2vkGL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we-zGs8lw7kY"
      },
      "source": [
        "#  1) Using `BeautifulSoup`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDQT-D171lhn"
      },
      "source": [
        "\n",
        "## Cook a soup\n",
        "\n",
        "- Connect to a website using`response = requests.get(url)`\n",
        "- Feed `response.content` into BeautifulSoup\n",
        "- Must specify the parser that will analyze the contents\n",
        "    - default available is `'html.parser'`\n",
        "    - recommended is to install and use `lxml` [[lxml documentation](https://lxml.de/3.7/)]\n",
        "- use soup.prettify() to get a user-friendly version of the content to print\n",
        "\n",
        "```python\n",
        "# Define Url and establish connection\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "response = requests.get(url, timeout=3)\n",
        "\n",
        "# Feed the response's .content into BeauitfulSoup\n",
        "page_content = response.content\n",
        "soup = BeautifulSoup(page_content,'lxml') #'html.parser')\n",
        "\n",
        "# Preview soup contents using .prettify()\n",
        "print(soup.prettify()[:2000])\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FieLZ63VVEXi"
      },
      "source": [
        "## What's in a Soup?\n",
        "- **A soup is essentially a collection of `tag objects`**\n",
        "    - each tag from the html is a tag object in the soup\n",
        "    - the tag's maintain the hierarchy of the html page, so tag objects will contain _other_ tag objects that were under it in the html tree.\n",
        "\n",
        "- **Each tag has a:**\n",
        "    - `.name`\n",
        "    - `.contents`\n",
        "    - `.string`\n",
        "    \n",
        "- **A tag can be access by name (like a column in a dataframe using dot notation)**\n",
        "    - and then you can access the tags within the new tag-variable just like the first tag\n",
        "    ```python\n",
        "    # Access tags by name\n",
        "    meta = soup.meta\n",
        "    head = soup.head\n",
        "    body = soup.body\n",
        "    # and so on...\n",
        "    ```\n",
        "- [!] ***BUT this will only return the FIRST tag of that type, to access all occurances of a tag-type, we will need to navigate the html family tree***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZCLylw9RkVL"
      },
      "source": [
        "\n",
        "## Navigating the HTML Family Tree: Children, siblings, and parents\n",
        "\n",
        "- **Each tag is located within a tree-hierarchy of parents, siblings, and children**\n",
        "    - The family-relation is based on the identation level of the tags.\n",
        "\n",
        "- **Methods/attributes for the location/related tags of a tag**\n",
        "    - `.parent`, `.parents`\n",
        "    - `.child`, `.children`\n",
        "    - `.descendents`\n",
        "    - `.next_sibling`, `.previous_sibling`\n",
        "\n",
        "- *Note: a newline character `\\n` is also considered a tag/sibling/child*\n",
        "\n",
        "#### Accessing Child Tags\n",
        "\n",
        "- To get to later occurances of a tag type (i.e. the 2nd `<p>` tag in a tree), we need to navigate through the parent tag's `children`\n",
        "    - To access an iterable list of a tag's children use `.children`\n",
        "        - But, this only returns its *direct children*  (one indentation level down)     \n",
        "        \n",
        "    ```python\n",
        "    # print direct children of the body tag\n",
        "    body = soup.body\n",
        "    for child in body.children:\n",
        "        # print child if its not empty\n",
        "        print(child if child is not None else ' ', '\\n\\n')  # '\\n\\n' for visual separation\n",
        "    ```\n",
        "- To access *all children* use `.descendents`\n",
        "    - Returns all chidren and children of children\n",
        "    ```python\n",
        "    for child in body.descendents:\n",
        "        # print all children/grandchildren, etc\n",
        "        print(child if child is not None else ' ','\\n\\n')  \n",
        "    ```\n",
        "    \n",
        "#### Accessing Parent tags\n",
        "\n",
        "- To access the parent of a tag use `.parent`\n",
        "```python\n",
        "title = soup.head.title\n",
        "print(title.parent.name)\n",
        "```\n",
        "\n",
        "- To get a list of _all parents_ use `.parents`\n",
        "```python\n",
        "title = soup.head.title\n",
        "for parent in title.parents:\n",
        "    print(parent.name)\n",
        "```\n",
        "\n",
        "#### Accessing Sibling tags\n",
        "- siblings are tags in the same tree indentation level\n",
        "- `.next_sibling`, `.previous_sibling`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXktQsDvWW0A"
      },
      "source": [
        "## Searching Through Soup\n",
        "\n",
        "\n",
        "### Finding the target tags to isolate\n",
        "Using example  from  [Wikipedia article](https://en.wikipedia.org/wiki/Stock_market)\n",
        "where we are trying to isolate the body of the article content.\n",
        "\n",
        "\n",
        "- **Examine the website using Chrome's inspect view.**\n",
        "\n",
        "    - Press F12 or right-click > inspect\n",
        "\n",
        "    - Use the mouse selector tool (top left button) to explore the web page content for your desired target\n",
        "        - the web page element will be highlighted on the page itself and its corresponding entry in the document tree.\n",
        "        - Note: click on the web page with the selector in order to keep it selected in the document tree\n",
        "\n",
        "    - Take note of any identifying attributes for the target tag (class, id, etc)\n",
        "<img src=\"https://drive.google.com/uc?export-download&id=1KifQ_ukuXFdnCh1Tz1rwzA_cWkB_45mf\" width=450>\n",
        "\n",
        "### Using BeautifulSoup's search functions\n",
        "Note: while the process below is a decent summary, there is more nuance to html/css tags than I personally have been able to digest.\n",
        "    - If something doesn't work as expected/explained, please verify in the documentation.\n",
        "        - [BeauitfulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautiful-soup-documentation)\n",
        "        - [docs for .find_all()](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all)\n",
        "    \n",
        "- **BeautifulSoup has methods for searching through descendent-tags**\n",
        "    - `.find`\n",
        "    - `.find_all`\n",
        "    \n",
        "- **Using `.find_all()`**\n",
        "    - Searches through all descendent tags and returns a result set (list of tag objects)\n",
        "```python\n",
        "# How to get results from .find_all()\n",
        "results = soup.find_all(name, attrs, recursive, string, limit,**kwargs) `\n",
        "```        \n",
        "    - `.find_all()` parameters:\n",
        "        - `name` _(type of tags to consider)_\n",
        "            - only consider tags with this name\n",
        "                - Ex: 'a',  'div', 'p' ,etc.\n",
        "        - `atrrs`_(css attributes that you are looking for in your target tag)_\n",
        "            - enter an attribute such as the class or id as a string\n",
        "\n",
        "                `attrs='mw-content-ltr'`\n",
        "            - if passing more than one attribute, must use a dictionary:\n",
        "\n",
        "            `attrs={'class':'mw-content-ltr', 'id':'mw-content-text'}`\n",
        "        - `recursive`_(Default=True)_\n",
        "            - search all children (`True`)\n",
        "            - search only  direct children(`False`)\n",
        "\n",
        "        - `string`\n",
        "            - search for text _inside_ of tags instead of the tags themselves\n",
        "            - can be regular expression\n",
        "        - `limit`\n",
        "            - How many results you want it to return\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "jJFr-RC7vzI9",
        "outputId": "4d0527cd-1220-466a-e08e-06a76259e94b"
      },
      "outputs": [],
      "source": [
        "# !pip install fake_useragent\n",
        "# !pip install lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUuCKNCdbYdS"
      },
      "source": [
        "# 2) Walk-through example/code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwhquyO3x9q7"
      },
      "source": [
        "    - James functions\n",
        "    - Functional code scraping wikipedia pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVfjE8RhHZq-"
      },
      "source": [
        "## James' Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xBlb5NuyeAC"
      },
      "source": [
        "- `soup = cook_soup_from_url(url)`\n",
        "    - make a beautiful soup from url\n",
        "-`soup_links = get_all_links(soup)`\n",
        "    - get all links from soup and return as a list.\n",
        "    \n",
        "-  `absolute_links = make_absolute_links(url, soup_links) `\n",
        "    - use If `soup_links` are relative links that do not include the website domain and start with '../' instead of 'https://www... ').\n",
        "    - then can use the `absolute_links` to make new soups to continue searching for your desired content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "GTXEdbjQ_7RD"
      },
      "outputs": [],
      "source": [
        "# def mount_google_drive(force_remount=True):\n",
        "#     from google.colab import drive\n",
        "#     print('drive_filepath=\"drive/My Drive/\"')\n",
        "#     return drive.mount('/content/drive', force_remount=force_remount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "aIolt_tWAQ-E",
        "outputId": "6b4491fd-f12b-4eea-a29b-726be19988de"
      },
      "outputs": [],
      "source": [
        "# mount_google_drive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XYgkNr2mA63V"
      },
      "outputs": [],
      "source": [
        "# drive_filepath=\"drive/My Drive/\"\n",
        "# # import os\n",
        "# # os.listdir(drive_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UwR64_fYv3Up"
      },
      "outputs": [],
      "source": [
        "def cook_soup_from_url(url, parser='lxml',sleep_time=0):\n",
        "    \"\"\"Uses requests to retreive webpage and returns a BeautifulSoup made using lxml parser.\"\"\"\n",
        "    import requests\n",
        "    from time import sleep\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    sleep(sleep_time)\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # check status of request\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f'Error: Status_code !=200.\\n status_code={response.status_code}')\n",
        "\n",
        "    c = response.content\n",
        "    # feed content into a beautiful soup using lxml\n",
        "    soup = BeautifulSoup(c,'lxml')\n",
        "    return soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4RM3tQkqyuoe"
      },
      "outputs": [],
      "source": [
        "def get_all_links(soup):#,attr_kwds=None):\n",
        "    \"\"\"Finds all links inside of soup that have the attributes(attr_kwds),which will be used in soup.findAll(attrs=attr_kwds).\n",
        "    Returns a list of links.\n",
        "    tag_type = 'a' or 'href'\"\"\"\n",
        "    all_a_tags = soup.findAll('a',attrs=kwds)\n",
        "    link_list = []\n",
        "    for link in all_a_tags:\n",
        "        test_link = link.get('href')#,attr=kwds)\n",
        "#         test_link = link.get('href',attrs=kwds)\n",
        "        link_list.append(test_link)\n",
        "    return link_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UNjG2XYFyPIH"
      },
      "outputs": [],
      "source": [
        "def make_absolute_links(source_url, rel_link_list):\n",
        "    \"\"\"Accepts the source_url for the source page of the rel_link_list and uses urljoin to return a list of valid absolute links.\"\"\"\n",
        "\n",
        "    from urllib.parse import urlparse, urljoin\n",
        "\n",
        "    absolute_links=[]\n",
        "\n",
        "    # Create a for loop to loop through links and make absolute html paths\n",
        "    for link in rel_link_list:\n",
        "\n",
        "        # Get base url using a url pasers and the story_url at the beginning of the nb\n",
        "        abs_link = urljoin(source_url,link)\n",
        "\n",
        "        #concatenate and append to a list\n",
        "        absolute_links.append(abs_link)\n",
        "\n",
        "    return absolute_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Zo1sJyNE6ZCb"
      },
      "outputs": [],
      "source": [
        "def cook_batch_of_soups(link_list, sleep_time=1): #,user_fun = extract_target_text):\n",
        "    \"\"\"Accepts a list of links to extract and save in a list of dictionaries of soups\n",
        "    with their relative url path as their key.\n",
        "    Set user_fun to None to just extract full soups without user_extract\"\"\"\n",
        "    from time import sleep\n",
        "    from urllib.parse import urlparse, urljoin\n",
        "\n",
        "    batch_of_soups = []\n",
        "\n",
        "    for link in link_list:\n",
        "        soup_dict = {}\n",
        "\n",
        "\n",
        "        # turn the url path into the dictionary key/title\n",
        "        url_dict_key_path = urlparse(link).path\n",
        "        url_dict_key = url_dict_key_path.split('/')[-1]\n",
        "\n",
        "        soup_dict['_url'] = link\n",
        "        soup_dict['path'] = url_dict_key\n",
        "\n",
        "        # make a soup from the current link\n",
        "        page_soup = cook_soup_from_url(link, sleep_time=sleep_time)\n",
        "        soup_dict['soup'] = page_soup\n",
        "\n",
        "\n",
        "#         if user_fun!=None:\n",
        "#             ## ADDING USER-SPECIFIED EXTRACTION FUNCTION\n",
        "#             user_output = user_fun(page_soup) #can add inputs to function\n",
        "#             soup_dict['user_extract'] = user_output\n",
        "\n",
        "        # Add current page's soup to batch_of_soups list\n",
        "        batch_of_soups.append(soup_dict)\n",
        "\n",
        "    return batch_of_soups\n",
        "\n",
        "\n",
        "def extract_target_text(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
        "    \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
        "\n",
        "    if attrs_dict==None:\n",
        "        found_tags = soup_or_tag.find_all(name=tag_name)\n",
        "    else:\n",
        "        found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
        "\n",
        "\n",
        "    # if extracting from multiple tags\n",
        "    output=[]\n",
        "    output = [tag.text for tag in found_tags if tag.text is not None]\n",
        "\n",
        "    if join_text == True:\n",
        "        output = ' '.join(output)\n",
        "\n",
        "    ## ADDING SAVING EACH\n",
        "    if save_files==True:\n",
        "        text = output #soup.body.string\n",
        "        filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
        "        soup_dict['filename'] = filename\n",
        "        with open(filename,'w+') as f:\n",
        "            f.write(text)\n",
        "        print(f'File  successfully saved as {filename}')\n",
        "\n",
        "    return  output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0ODwEjKH92T5"
      },
      "outputs": [],
      "source": [
        "def pickled_soup(soups, save_location='./', pickle_name='exported_soups.pckl'):\n",
        "    import pickle\n",
        "    import sys\n",
        "\n",
        "    filepath = save_location+pickle_name\n",
        "\n",
        "    with open(filepath,'wb') as f:\n",
        "        pickle.dump(soups, f)\n",
        "\n",
        "    return print(f'Soup successfully pickled. Stored as {filepath}.')\n",
        "\n",
        "def load_leftovers(filepath):\n",
        "    import pickle\n",
        "\n",
        "    print(f'Opening leftovers: {filepath}')\n",
        "\n",
        "    with open(filepath, 'rb') as f:\n",
        "        leftover_soup = pickle.load(f)\n",
        "\n",
        "    return leftover_soup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng-HN_rRymjI"
      },
      "source": [
        "## Walkthrough - using James' functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1Pg3VWWmypij",
        "outputId": "1b76df53-a624-4173-8269-f96029873080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/wiki/Collateralised_debt_obligation', '/wiki/Performance_bonds', '/wiki/Corporate_tax_haven', '/wiki/Credit_(finance)', '/wiki/Government_operations']\n",
            "['https://en.wikipedia.org/wiki/Collateralised_debt_obligation', 'https://en.wikipedia.org/wiki/Performance_bonds', 'https://en.wikipedia.org/wiki/Corporate_tax_haven', 'https://en.wikipedia.org/wiki/Credit_(finance)', 'https://en.wikipedia.org/wiki/Government_operations']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "from fake_useragent import UserAgent\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "soup = cook_soup_from_url(url,sleep_time=1)\n",
        "\n",
        "\n",
        "## Get all links that match are interal wikipedia redirects [yes?]\n",
        "kwds = {'class':'mw-redirect'}\n",
        "links = get_all_links(soup)#,kwds)\n",
        "\n",
        "\n",
        "# preview first 5 links\n",
        "print(links[:5])\n",
        "\n",
        "\n",
        "# Turn relative links into absolute links\n",
        "abs_links = make_absolute_links(url,links)\n",
        "print(abs_links[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "6tCwXLiN7UCk",
        "outputId": "d360207c-aa0f-40eb-e8f3-de1d0d90b6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of input links: == # of soups in batch:\n",
            "5 == 5\n",
            "\n",
            "Each soup_dict has  dict_keys(['_url', 'path', 'soup'])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Selecting only the first 5 links to test\n",
        "abs_links_for_soups = abs_links[:5]\n",
        "\n",
        "\n",
        "# Cooking a batch of soups from those chosen links\n",
        "batch_of_soups = cook_batch_of_soups(abs_links_for_soups, sleep_time=2)\n",
        "\n",
        "# batch_of_soups is a list as long as the input link_list\n",
        "print(f'# of input links: == # of soups in batch:\\n{len(abs_links_for_soups)} == {len(batch_of_soups)}\\n')\n",
        "\n",
        "# batch_of_soups is a list of soup-dictionaries\n",
        "soup_dict = batch_of_soups[0]\n",
        "print('Each soup_dict has ',soup_dict.keys())\n",
        "\n",
        "# the page's soup is stored under soup_dict['soup']\n",
        "soup_from_soup_dict = soup_dict['soup']\n",
        "type(soup_from_soup_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFVnj20YmQK2"
      },
      "source": [
        "#### Notes on extracting content.\n",
        "- Edit the `extract_target_text function` in the James' functions settings or uncomment and use the `extract_target_text_custom function` below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "QwU8aPHJhAVm",
        "outputId": "034b20bf-67f1-4f4f-8450-33ee0f4ab111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A collateralized debt obligation (CDO) is a type of structured asset-backed security (ABS).[1] Originally developed as instruments for the corporate debt markets, after 2002 CDOs became vehicles for refinancing mortgage-backed securities (MBS).[2][3] Like other private label securities backed by assets, a CDO can be thought of as a promise to pay investors in a prescribed sequence, based on the cash flow the CDO collects from the pool of bonds or other assets it owns.[4] Distinctively, CDO credit risk is typically assessed based on a probability of default (PD) derived from ratings on those bonds or assets.[5]\n",
            " The CDO is \"sliced\" into sections known as \"tranches\", which \"catch\" the cash flow of interest and principal payments in sequence based on seniority.[6] If some loans default and the cash collected by the CDO is insufficient to pay all of its investors, those in the lowest, most \"junior\" tranches suffer losses first.[7] The last to lose payment from default are the safest, most \n"
          ]
        }
      ],
      "source": [
        "## ADDING extract_target_text to precisely target text\n",
        "# def extract_target_text_custom(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
        "#     \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
        "\n",
        "#     if attrs_dict==None:\n",
        "#         found_tags = soup_or_tag.find_all(name=tag_name)\n",
        "#     else:\n",
        "#         found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
        "\n",
        "\n",
        "#     # if extracting from multiple tags\n",
        "#     output=[]\n",
        "#     output = [tag.text for tag in found_tags if tag.text is not None]\n",
        "\n",
        "#     if join_text == True:\n",
        "#         output = ' '.join(output)\n",
        "\n",
        "#     ## ADDING SAVING EACH\n",
        "#     if save_files==True:\n",
        "#         text = output #soup.body.string\n",
        "#         filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
        "#         soup_dict['filename'] = filename\n",
        "#         with open(filename,'w+') as f:\n",
        "#             f.write(text)\n",
        "#         print(f'File  successfully saved as {filename}')\n",
        "\n",
        "#     return  output\n",
        "\n",
        "# ####################\n",
        "\n",
        "def extract_target_text_custom(soup_or_tag, tag_name='p', attrs_dict=None, join_text=True, save_files=False):\n",
        "    \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
        "\n",
        "    if attrs_dict is None:\n",
        "        found_tags = soup_or_tag.find_all(name=tag_name)\n",
        "    else:\n",
        "        found_tags = soup_or_tag.find_all(name=tag_name, attrs=attrs_dict)\n",
        "\n",
        "    # Extracting text from found tags\n",
        "    output = [tag.text for tag in found_tags if tag.text is not None]\n",
        "\n",
        "    if join_text:\n",
        "        output = ' '.join(output)\n",
        "\n",
        "    # Adding saving each\n",
        "    if save_files:\n",
        "        text = output\n",
        "        filename = f\"text_extract_{url_dict_key}.txt\"\n",
        "        soup_dict['filename'] = filename\n",
        "        with open(filename, 'w+') as f:\n",
        "            f.write(text)\n",
        "        print(f'File successfully saved as {filename}')\n",
        "\n",
        "    return output\n",
        "\n",
        "# RUN A LOOP TO ADD EXTRACTED TEXT TO EACH SOUP IN THE BATCH\n",
        "for i, soup_dict in enumerate(batch_of_soups):\n",
        "\n",
        "    # Get the soup from the dict\n",
        "    soup = soup_dict['soup']\n",
        "\n",
        "    # Extract text using the custom function\n",
        "    extracted_text = extract_target_text_custom(soup)\n",
        "\n",
        "    # Add key:value for results of extract\n",
        "    soup_dict['extracted'] = extracted_text\n",
        "\n",
        "    # Replace the old soup_dict with the new one with 'extracted'\n",
        "    batch_of_soups[i] = soup_dict\n",
        "\n",
        "# Example to check the result\n",
        "example_extracted_text = batch_of_soups[0]['extracted']\n",
        "print(example_extracted_text[:1000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2lgbOyKyg0-"
      },
      "source": [
        "___\n",
        "___\n",
        "\n",
        "# Walk-through from Study Group (06/24/19):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eUJ50zdSvjyL"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# from fake_useragent import UserAgent\n",
        "# ua = UserAgent()\n",
        "\n",
        "# header = {'user-agent':ua.chrome}\n",
        "# print('Header:\\n',header)\n",
        "\n",
        "# url ='https://en.wikipedia.org/wiki/Stock_market'\n",
        "# response = requests.get(url, timeout=3, headers=header)\n",
        "\n",
        "# print('Status code: ',response.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJwSP_bWnBL3"
      },
      "source": [
        "#### Example For Kate's Website\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "texdGvNkOoY8"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "Error: Status_code !=200.\n status_code=404",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://www.temis.nl/uvradiation/archives/v2.0/overpass/uv_Bern_Switzerland.dat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m batch_soups_kate \u001b[38;5;241m=\u001b[39m \u001b[43mcook_batch_of_soups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43murl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m## Saving each page's body as a text file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m soup_dict \u001b[38;5;129;01min\u001b[39;00m batch_soups_kate:\n",
            "Cell \u001b[1;32mIn[50], line 22\u001b[0m, in \u001b[0;36mcook_batch_of_soups\u001b[1;34m(link_list, sleep_time)\u001b[0m\n\u001b[0;32m     19\u001b[0m         soup_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m url_dict_key\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# make a soup from the current link\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m         page_soup \u001b[38;5;241m=\u001b[39m \u001b[43mcook_soup_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msleep_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m         soup_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoup\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m page_soup\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#         if user_fun!=None:\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#             ## ADDING USER-SPECIFIED EXTRACTION FUNCTION\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#             user_output = user_fun(page_soup) #can add inputs to function\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#             soup_dict['user_extract'] = user_output\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m         \u001b[38;5;66;03m# Add current page's soup to batch_of_soups list\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[47], line 12\u001b[0m, in \u001b[0;36mcook_soup_from_url\u001b[1;34m(url, parser, sleep_time)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# check status of request\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError: Status_code !=200.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m status_code=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m c \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# feed content into a beautiful soup using lxml\u001b[39;00m\n",
            "\u001b[1;31mException\u001b[0m: Error: Status_code !=200.\n status_code=404"
          ]
        }
      ],
      "source": [
        "url='http://www.temis.nl/uvradiation/archives/v2.0/overpass/uv_Bern_Switzerland.dat'\n",
        "batch_soups_kate = cook_batch_of_soups([url])\n",
        "\n",
        "## Saving each page's body as a text file\n",
        "for soup_dict in batch_soups_kate:\n",
        "    text = soup_dict['soup'].body.string\n",
        "    filename =f\"drive/My Drive/test_text_saving {soup_dict['url_dict_key']}.txt\"\n",
        "    with open(filename,'w+') as f:\n",
        "        f.write(text)\n",
        "\n",
        "## Loading in a file to test if working.\n",
        "test_file = batch_soups_kate[0].filename\n",
        "with open(filename,'r') as f:\n",
        "    data = f.read()\n",
        "print(data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
